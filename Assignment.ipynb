{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42009dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\vsgha\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\vsgha\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2be790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_file = open('dharamsotu2019.pdf', 'rb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04aab224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PdfFileReader object\n",
    "pdf = PdfReader(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5658b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    " # Get the number of pages\n",
    "num_pages = len(pdf.pages)\n",
    "print(num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff05352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 10\n"
     ]
    }
   ],
   "source": [
    "# Print the number of pages\n",
    "print(f'Number of pages: {num_pages}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffc9da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Sampling for Visualization of Dynamic data\n",
      "using LION-tSNE\n",
      "Bheekya Dharamsotu∗, K. Swarupa Rani†, Salman Abdul Moiz‡and C. Raghavendra Rao§\n",
      "School of Computer and Information Sciences\n",
      "University of Hyderabad\n",
      "Hyderabad, India\n",
      "Email:∗15mcpc21@uohyd.ac.in,†kswarupaprasad@gmail.com,‡salman.abdul.moiz@gmail.com,§crrcs.ailab@gmail.com\n",
      "Abstract —Dimensionality reduction algorithms are often used\n",
      "to visualize multi-dimensional data, which are mostly non-\n",
      "parametric. Non-parametric methods do not provide any explicitintuition for adding new data points into an existing environmentwhich limits the applicability of visualization for Big Datascenario. The LION-tSNE (Local Interpolation with OutliercoNtrol t-Distributed Stochastic Neighbor Embedding) methodwas proposed to overcome the limitations of existing techniques.The LION-tSNE algorithm uses random sampling method fortSNE model design which creates an initial visual environmentthen new data points are added to this environment usinglocal-IDW(Inverse Distance Weighting) interpolation method.The randomly selected sample data often suffer from non-representativeness of the whole data which creates inconsistencyin the tSNE environment. To overcome this problem two newsampling methods are proposed which are based on k-NN (k -\n",
      "Nearest Neighbor) graph update properties. It is empiricallyshown that proposed methods outperform existing LION-tSNEmethod with 0.5 to 2% more k-NN accuracy and results are more\n",
      "consistent. The study is done on ﬁve differently characterizeddatasets with three different initial solutions of tSNE. Theproposed method results are statistically signiﬁcant which is doneby statistical method pairwise t-test.\n",
      "Index T erms—dimensionality reduction, visualization, Big\n",
      "Data, t-Distributed Stochastic Neighbor Embedding(tSNE), in-terpolation, sampling, k-NN graph\n",
      "I. I NTRODUCTION\n",
      "Exploratory analysis of multi-dimensional data is a prob-\n",
      "lem that arises in many areas of science such as ma-\n",
      "chine learning, data mining, visualization, etc. Visualiza-tion plays a paramount role in the exploratory analysis ofmulti-dimensional data. The visual representation of multi-dimensional data provides an intuitive interface for an analystto detect the underlying structure such as homogeneous re-gions, clusters and outliers of complex data. In visualization,the extraction of the underlying structure of multi-dimensionaldata is completely relying on the cognitive capabilities ofthe data analyst. A data analyst can percept only either 2Dor 3D visual representation of the data. Obtaining 2D visualrepresentation of multi-dimensional data is not a simple task.From the past several years, researchers have proposed varioustechniques to reduce the dimensionality and to visualize themulti-dimensional data. The Dimensionality Reduction (DR)[1], [2], [3] for visualization provides an opportunity to thedecision makers or analysts to visualize the high-dimensionaldata in a low-dimensional space which provides ﬂexibilityto make effective decisions. The developed DR techniquesdiffer with the type of structure preserved by them. The linearDR techniques such as Principal Component Analysis (PCA)[4], Factor Analysis (FA) [5], and Multi-Dimensional Scaling(MDS) [6] are used to keep two dissimilar data points farawayfrom each other in low-dimensional space. These techniquesare incapable of reducing dimension to low-dimensional spacedue to the non-linearity in the multidimensional data. Incontrast to linear DR, Non-linear DR techniques such asIsomap [7], Sammon mapping [8], Curvilinear ComponentAnalysis (CCA) [9], Local Linear Embedding (LLE) [10],Laplacian Eigenmap (LE) [11], Maximum V ariance Unfold-ing (MVU) [12] and Stochastic Neighbor Embedding (SNE)[13] are proposed to handle non-linear data. In both theapproaches feature extraction depends on the characteristicsof interest in the data: inter-point distances, variation, linearsubspace, geodesic distances, reconstruction weights, lineartangent space, neighborhood graph and conditional probabilitydistribution.\n",
      "Student t-Distributed Stochastic Neighbor Embedding\n",
      "(tSNE) [14] is a popular DR technique for visualization.The main strength of tSNE is to preserve the underlyingstructure of multi-dimensional data in 2D or 3D space by pre-serving the neighborhood property of high-dimensional datain low-dimensional space. Hence, the homogeneous regionsand clusters of high-dimensional data should be visible inthe 2D or 3D space of tSNE. Over the last decade, tSNEhas been successfully applied to visualize many applicationssuch as genomic data [15], healthcare informatics [16], tumorsubpopulations [17], etc.\n",
      "Apart from classical technique Principal Component Anal-\n",
      "ysis (PCA) and Self-Organizing Map (SOM) [18], most ofthe DR are non-parametric techniques. The characteristics ofthese techniques includes:(i) to only provide the projection ofgiven data samples. (ii) do not provide any precise intuitionfor adding new data samples into the existing visualization,and (iii) very ﬂexible and computationally fast. Hence, non-parametric techniques are not suitable for visualizing the timeseries and streaming data. Non-parametric techniques doesnot allow the visualization of parts of a given data and thenscalable to Big Data sets on demand. Most of the existingtechniques require at least quadratic computational complexitywith respect to data set size which causes suffering from\n",
      "632019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)\n",
      "2640-0316/19/$31.00 ©2019 IEEE\n",
      "DOI 10.1109/HiPC.2019.00019\n",
      "\n",
      "scalability issues.\n",
      "Over the last years, researchers attempted to extend tSNE\n",
      "by adding new data [19], [20], [21] into existing tSNE\n",
      "representation. In this paper, we have proposed a new sam-pling technique called k-NN Sampling to extend the Local\n",
      "Interpolation with Outlier coNtrol tSNE (LION-tSNE) [22]technique. The LION-tSNE is a novel approach for handlingthe addition of new data samples in two ways. Firstly, theInverse Distance Weight Interpolation (i.e., IDW-Interpolation)algorithm [22] is used to include the inlier data into the trainedtSNE model based on local neighborhood. The local neighbor-hood is obtained from distance radius which is derived fromthe tSNE model. And the second one is Outlier\n",
      "Placement\n",
      "algorithm [22] which used to compute the free location forplacing the outliers and provides the heuristic to maintain theminimum distance between new and existing outliers. Whileadding new data points in a tSNE environment, the identiﬁedoutliers which does not have any relation with outliers oftSNE are placed randomly in a predeﬁned location. In LION-tSNE model, the random sampling method used for selectinga training sample from the whole population with a givenﬁxed sample size(i.e., two-thirds of the given data). Randomsampling method suffers from inefﬁcient representativeness ofthe complete data that causes lots of variation in result fromone execution to another, which refers to the inconsistency inthe results. In this paper, we are addressing the problem ofnon-representativeness of the random sampling method.\n",
      "The organization of the paper is as follows. In section II, we\n",
      "are discussing the preliminaries which provide a requirementfor understanding the basics of this paper. Section III describesthe related work. In section IV , we are giving a detaileddescription of the proposed two k-NN sampling approaches. In\n",
      "section V , we are providing the result analysis, evaluation, andcomparison between proposed two k-NN sampling method\n",
      "results and the state-of-the-art technique LION-tSNE. Finally,section VI gives the direction for future work and conclusion.\n",
      "II. P\n",
      "RELIMINARY\n",
      "A. Student t- Distributed Stochastic Neighbor Embedding(tSNE)\n",
      "The tSNE algorithm developed by Laurens van der Maaten\n",
      "and Geoffrey Hinton [14] in 2008, based on the SNE [13] al-gorithm. The main idea of SNE algorithm is to represent multi-dimensional data into a human perceptual low-dimensionalspace. The low-dimensional representation of SNE shouldpreserve the underlying structure of high-dimensional data inlow-dimensional space by maintaining a similar neighborhoodproperty of original data. Initially, the tSNE algorithm convertsthe distance between the pair of points into a conditional prob-ability distribution. The probability distribution measures thesimilarity between a pair of points. The divergence betweenthe similarities of high-dimensional data and low-dimensionalembedding is measured by Kullback-Leibler divergence (i.e.,KL-divergence). The minimization of the KL-divergence isobtained by the simple gradient descent that would be anobjective of an SNE algorithm. The workﬂow of tSNE isshown in the ﬁgure 1.\n",
      "6\u001b -\n",
      "- -? ?Update\n",
      "Output Embedding\n",
      "Pairwise Similarity Matrix (P)Compute High-Dimensional Compute Gradient Decent of Compute\n",
      "KLDIV (P||Q) KLDIV (P||Q)Output Embedding Y={y1, ..., yN}\n",
      "Where yi∈Rd\n",
      "Input Dataset X={x1, ...., xN}\n",
      "where xi∈RmCompute Low-Dimensional\n",
      "Pairwise Similarity Matrix(Q)\n",
      "Repeat this process until convergence or maximum number of iterations\n",
      "Figure 1: The tSNE workﬂow model\n",
      "Lets assume we are given an input dataset X =\n",
      "{x1,x2,...x N}where each xi∈RDis a D-dimensional\n",
      "vector. The given input data can be transformed to Y=\n",
      "{y1,y2...y N}where each yi∈Rdis d-dimensional vector,\n",
      "d/lessmuchD. The function d(xi,xj)measures the distance between\n",
      "pair of points. The similarity between pair of input pointsx\n",
      "iandxjis denoted by pj/i. The value of pj/i is the\n",
      "probability of choosing xjas neighbor of xi. The neighbors\n",
      "ofxiwere chosen in proportion to their probability density\n",
      "under a Gaussian distribution with center xi. For instance,\n",
      "pj/iis relatively high for nearby data points and inﬁnitesimal\n",
      "for faraway points. The pj/iand symmetric pijis deﬁned by\n",
      "pj/i=exp/parenleftbigg\n",
      "−d(xi,xj)2\n",
      "2σi2/parenrightbigg\n",
      "/summationtextN\n",
      "k/negationslash=iexp/parenleftBig−d(xi,xk)2\n",
      "2σi2/parenrightBig,pi/i=0,pij=pj/i+pi/j\n",
      "2N(1)\n",
      "Where the σiis the variance which can be obtained by\n",
      "binary search using perplexity (μ) as a given parameter. The\n",
      "perplexity is a smooth measure of an effective number ofneighbors for each data point. The density of the data isdifferent across Euclidean space, therefore we need differentσfor each input x\n",
      "i. The probability distribution Piofxiis\n",
      "obtained based on the σivalue. The tSNE visual representation\n",
      "completely depends on the given parameter perplexity. Theperplexity μis obtained by Shannon entropy of P\n",
      "iwhich gives\n",
      "the information gain in Information Retrieval System (IRS).When there is an increment in σ\n",
      "i, then there is a relative\n",
      "increment in entropy. The μand entropy is given by\n",
      "μ=2H(Pi)where H (Pi)=−N/summationdisplay\n",
      "jpj/ilog2pj/i (2)\n",
      "In low-dimensional embedding space, map-points yiandyj\n",
      "are the representative of corresponding multi-dimensional datapointsx\n",
      "iandxj, the similarity between yiandyjis denoted\n",
      "byqij. Theqijis deﬁned by\n",
      "qij=(1 +d(yi,yj)2)−1\n",
      "/summationtextN\n",
      "l/summationtextNk/negationslash=l(1 +d(yl,yk)2)−1,qii=0 (3)\n",
      "Here student t-distribution is used instead of Gaussian distri-\n",
      "bution. Gaussian distribution of low-dimensional embeddingsuffers with crowding problem [23].\n",
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ifpij∼qij,∀i/negationslash=j∈N, then the given data is efﬁciently\n",
      "embedded into the low-dimensional space. Otherwise, com-\n",
      "pute the KL-divergence (i.e., error) between pijandqijthat\n",
      "is equal to the cross-entropy in IRS. The cost function (C )o r\n",
      "objective of tSNE is given by\n",
      "C=KL(P/bardblQ)=N/summationdisplay\n",
      "jpijlogpij\n",
      "qij(4)\n",
      "The cost function Cis optimized by simple gradient descent\n",
      "method which is given by\n",
      "∂C\n",
      "∂yi=4/summationdisplay\n",
      "j(pij−qij)qijZ(yi−yj) (5)\n",
      "whereZ=( 1+d(yi,yj)2)−1is a normalization term of stu-\n",
      "dent t-distribution. The updatation of map-points are obtainedby applying the equation 5 which computes the attractive and\n",
      "repulsive forces applied among map points. If two map pointsare similar, then the attractive force applied; otherwise, therepulsive force applied. The optimization of cost function C\n",
      "is suffering from local minima problem which can be resolvedby applying early exaggeration [14] to input similarity. Afterseveral simple gradient descent iterations, the tSNE modelconverges the underlying structure of the input data.\n",
      "B. Sampling Clustering (SC)\n",
      "Sampling Clustering (SC) [28] constructs a lite informative\n",
      "non-binary dendrogram by recursively dividing a graph into\n",
      "subgraphs until there are no vertices in the graph. In eachrecursive call, the graph nodes are sampled to disconnect latentsub-clusters. The sampled nodes are removed from the graphalong with edges. After each sampling, condense is applied toavoid the fragmentation of clusters; this allows the addition ofnew edges to remaining nodes by connecting the other nearestneighbor points of a graph. The process of sampling is showninﬁgure 2. This work motivated us to propose new sampling\n",
      "methods.\n",
      "C. Inverse Distance Weight Interpolation (IDW-Interpolation)\n",
      "IDW-Interpolation [26] map new data point x∈R\n",
      "Dinto\n",
      "the existing embedding yi∈Rd, where{i=1,2,.......m }\n",
      "andm<N . IDW-Interpolation determine the value of xas\n",
      "weighted sum of values yi, where weight is proportional to\n",
      "inverse distances. The IDW-Interpolation of xis given by\n",
      "IDWI (x)=/summationdisplay\n",
      "/bardblx−xi/bardbl≤rwi(x).yi,\n",
      "Where w i(x)=/bardblx−xi/bardbl−p\n",
      "/summationtext\n",
      "/bardblx−xi/bardbl≤r/bardblx−xi/bardbl−p(6)\n",
      "For instance, when the data point x→xi, the inverse\n",
      "distance /bardblx−xi/bardbl−1→∞ , the corresponding weight\n",
      "wi(x)→1( i.e.,∀j/negationslash=iwj(x)→0due to the normalization)\n",
      "andIDWI (x)→yi. In our expermentation we use local\n",
      "IDW-Interpolation which consider only limited number of\n",
      "neighbor points for computing the value of x. The neighbor\n",
      "points selection is done by given parameter radius rxNN . Theparameter rxNN value is calculated by the heuristic proposed\n",
      "by Andrey Boytsov et.al. [22]. In local IDW-Interpolation,the power parameter pplays an important role. For instance,\n",
      "very small value of ppredict the value of xaround the\n",
      "center:y≈mean(y\n",
      "i)(unlessx=xi) even the distance\n",
      "/bardblx−xi/bardblis low because the weight distribution is close to\n",
      "uniform. When the power parameter is high and the distance/bardblx−x\n",
      "i/bardblis low, the weight wi(x)of very ﬁrst nearest\n",
      "neighbor is dominating all other neighbors, therefore y≈yi\n",
      "wherei=argmin/bardblx−xj/bardbl. Too small and too large values\n",
      "of p suffers with different forms of overﬁtting. In LOIN-tSNE[22], authors proposed a generalization of power parameterby using leave-one-out cross-validation which is applied ontraining sample. Based on leave-one-out cross validation theycomputed the local IDW-Interpolation for each training samplethat produce the estimation of each y\n",
      "i. Afterwards, the mean\n",
      "square distance between estimated yi’s and real yi’s is com-\n",
      "puted. While optimizing power parameter the mean squareerror should be minimum. The obtained power parameterconsidered as a metric. However, this metric is heuristic, notan exact criteria.\n",
      "D. (t,m,s)-Nets and (t,s)-Sequences\n",
      "The (t,m,s) -Nets and (t,s)-sequences [24] combines the\n",
      "Jittered and Latin Hypercube sampling in order to achieve\n",
      "more uniformity and general concepts of stratiﬁcation (seeﬁgure 3(b)). Jittered sampling divides the data space into Nequal sized cubes then select one sample from each cuberandomly, where N=n×mandn∼min 2D coordinated\n",
      "dataset(see ﬁgure 3(a)). In Latin hypercube sampling eachdata coordinate is divided into N equal intervals. Then thesamples are chosen randomly such that each interval containsexactly one point (see ﬁgure 3(c)). For instance, 16-sample2D representation of Jittered, Latin Hypercube and (0,4,2)-\n",
      "Nets samplings is shown in ﬁgure 3.I n ﬁgure 3, (t,m,s) -\n",
      "Nets impose more stratiﬁcation for uniform distribution acrossthe given population. Therefore, the representativeness of thepopulation is maintained more uniformly.\n",
      "III. R\n",
      "ELA TED WORK\n",
      "To incorporate new data into the existing tSNE environment,\n",
      "most of the existing technique designed a mapping functionf:X→Y , which accepts multi-dimensional data and returns\n",
      "its low-dimensional embedding. Obtained mapping functionsare used for incorporating the new data points into the tSNEenvironment.\n",
      "Laurens van der Matten [19], the author of the original tSNE\n",
      "algorithm, has proposed parametric tSNE that learn both tSNErepresentation and mapping together. The mapping functionof parametric tSNE is obtained by neural network conceptRestricted Boltzmann Machines(RBM) [29] which is used forbuilding an approximation of tSNE mapping. Gisbrecht et.al.[20] proposed Kernel-tSNE which has close relation to RBF-Interpolation [25] and IDW-Interpolation [26]. Kernel-tSNE isa parametric tSNE designed based on the normalized Gaussiankernels.\n",
      "65\n",
      "Figure 2: An ideal ﬂow diagram of sampling cluster. First the dataset is converted into a graph. The graph is sampled, condensed and partitioned into small\n",
      "graphs.The same operations are performed until the termination condition is reached.\n",
      "Figure 3: Realization of (a) Jittered sampling (b) (0,4,2)-Nets and (c) Latin\n",
      "Hypercube sampling\n",
      "Pezotti et.al. [21] proposed Approximated-tSNE for Pro-\n",
      "gressive Visual Analytics. This algorithm approximates the\n",
      "distance function using k-Nearest Neighbors for tSNE embed-ding and also used the degree of approximation to show thetrade-off between embedding quality and computation speedup.\n",
      "Andrey Boytsov et.al. [22] proposed LION-tSNE algorithm\n",
      "based on local IDW-Interpolation for adding new data sampleinto an existing tSNE environment. It also addresses theoutlier handling approaches. In this paper, we are extend-ing the idea of LION-tSNE algorithm by proposing k-NN\n",
      "sampling method for the selection of training samples. Itallows the samples selection with respect to their k-nearestneighbors instead of random sampling for tSNE model design.For more efﬁcient sampling in the context of visualizationreview [34] which selects sample according to the speciﬁedgroup constraint preservation. In our proposed approach, weare choosing samples according to their preference which iscalculated by neighborhood property of each data points. Inour paper, we are addressing the lack of representativenessproblem which is caused by random sampling in LION-tSNE.Therefore, in our experimental evaluation, we are comparingproposed methods results with only LION-tSNE. For detailedcomparative study, refer Andrey Boytsov et.al. [22] paper.\n",
      "IV . P\n",
      "ROPOSED METHOD\n",
      "The framework of the proposed method is shown in the\n",
      "ﬁgure 4. It has four stages, at stage 1 data preprocessingwill be done such as the removal of redundant data pointsand ﬁlling the empty variables with appropriate values. Atstage 2, the proposed k-NN sampling is done for selectingtrain\n",
      "sample which is described in section A. At stage 3,\n",
      "the selected train samples projected into a low-dimensional\n",
      "embedding with Barnes-Hut tSNE (BH-tSNE) [27] and newdata points are added into the tSNE model that discussed insection B. At Last k-NN accuracy is calculated for whole data\n",
      "as discussed in section C.\n",
      "- - -\n",
      "?Data Preprocessing\n",
      "k-NN Accuracy1.Interpolating new dataLION-tSNE forInput\n",
      "Data\n",
      "1. Remove redundencytrain sample selection\n",
      "2.Outlier handlingDesign tSNE model on\n",
      "train sample to projectk-NN Sampling for\n",
      "from HD to LD\n",
      "Figure 4: Proposed framework of k-NN sampling for visualization of dynamic\n",
      "data using LION-tSNE\n",
      "A. k- Nearest Neighbor sampling (k-NN sampling)\n",
      "In our proposed k-NN sampling method, we are comput-\n",
      "ing Nearest Neighbor score (NN score) and Mutual Nearest\n",
      "Neighbor score (MNN score) from the k-NN graph. Lets\n",
      "assumek-NN graph is a directed graph G=(V,E), the edge\n",
      "E(v1,v2)givesv2as a neighbor of v1and neighborhood of v1\n",
      "is denoted by Nv1. The out-degree of each vertex is equal to\n",
      "k, and the in-degree of a vertex depends on the neighborhood\n",
      "property of other vertices. In our method, each data point is\n",
      "considered as a vertex of the k-NN graph and kis taken as\n",
      "a parameter for selecting the optimal training sample. TheNN\n",
      "score of data point xiis equal to the in-degree ofxi\n",
      "which is given by\n",
      "NNscore (xi)=|{xj|xi∈Nxj}|,∀j/negationslash=ixj∈X (7)\n",
      "whereXdenotes whole data set, Nxjdenotes the neighbor-\n",
      "hood ofxj. The MNN score of data point xiis at most k\n",
      "which is given by\n",
      "MNNscore(x i)=|{xj|xi∈Nxj∧xi|xj∈Nxi}|,∀j/negationslash=ixj∈X\n",
      "(8)\n",
      "66\n",
      "The data point xiis selected as a train sample and which can\n",
      "have representativeness of its neighbors is given by\n",
      "trainsample =firstindex{argmax xi∈X\n",
      "{NNscore(xi)}∩argmax xi∈X{MNN score(xi)}} (9)\n",
      "Proposed k-NN sampling algorithm shown in algorithm 1.\n",
      "Initially, the train sample is a null set; at each iteration, one\n",
      "data point is appended to the train sample. At each iteration,\n",
      "train sample as well as its mutual neighbors, are deleted from\n",
      "theXand then k-NN graph is updated. For k-NN graph\n",
      "updation we are proposing two different strategies, one is\n",
      "statick-NN graph updation and another one is dynamic k-NN\n",
      "graph updation. The train samples are selected according to\n",
      "the above two methods. In both approaches, the train sample\n",
      "selection is repeating until the NN score of remaining Xis\n",
      "equal to zero or size of remaining Xis less than kofk-NN\n",
      "graph. Later, if the size of the remaining Xis equal to zero,\n",
      "then the selected train sample represents the complete data,\n",
      "and it is considered to be good. Otherwise, the data pointsare sampled from the remaining Xusing (t,m,s)-Nets. The\n",
      "samples which are selected using (t,m,s)-Nets are appendedto the train\n",
      "sample for improving the representativeness of\n",
      "the data. The computational complexity of proposed methodisO(N\n",
      "2)which limits the algorithm scalable to large datasets.\n",
      "To get the effective insights from the obtained NN score’s and\n",
      "MNN score’s we would like to visualize the k-NN graphs of\n",
      "proposed methods with terrain metaphor [33] which can bedone in future work. The embedding of selected train\n",
      "sample\n",
      "and addition of new data into an existing tSNE environmentis described in the following section.\n",
      "B. Low-dimensional embedding of data\n",
      "1) tSNE model: train\n",
      "sample embedding: Barnes-Hut\n",
      "tSNE (BH-tSNE) [27] algorithm is used to generate low-\n",
      "dimensional embedding of selected train sample data. BH-\n",
      "tSNE is an optimized version of tSNE; it optimizes thetSNE objective function by input similarity approximationand gradient descent approximation. The original BH-tSNEalgorithm takes initial embedding points randomly and thenapplies early exaggeration on original data. The process ofearly exaggeration is used to move two similar points near toeach other in embedding space. The random initial solutiontakes more number of iteration for convergence. To overcomethis problem, in our experimentation, we are consideringtwo more initial solutions such as PCA and MDS for betteraccuracy and to reduce the divergence cost. For adding newdata points into a tSNE model is discussing in the belowsection.\n",
      "2) LION-tSNE: Interpolation and Outlier handling: The\n",
      "new data points are added to the tSNE model in two ways.The new data point addition will be done according to thecalculated parameter, r\n",
      "xNN ,ryNN andrclose . The parameter\n",
      "rxNN is the minimum radius of input data that decides whether\n",
      "the given new data point is either inlier or outlier based onthe heuristic proposed by Andrey Boytsov et.al [22]. TheseAlgorithm 1: k-NN Sampling algorithm\n",
      "Data: data setX={x1,x2,.....xN}, parameter k for minimal\n",
      "train sample\n",
      "Result: Return train sample\n",
      "trainsample =∅\n",
      "begin\n",
      "Compute k-NN graph of X\n",
      "repeat\n",
      "Compute NNscore (X)\n",
      "Compute MNNscore(X )\n",
      "index =[NNscore(X )= =\n",
      "argmax{NN score(X )}]/*gives all index\n",
      "which are having same NN-score */\n",
      "iflen(index )>1then\n",
      "trainindex =argmax{MNN score(x i)}where\n",
      "i∈index\n",
      "end\n",
      "else\n",
      "trainindex =index\n",
      "endtrain\n",
      "sample =trainsample∪X[index ]\n",
      "Determine the mutual neighbors of index\n",
      "Delete the selected index as well as its mutual neighbors\n",
      "fromX\n",
      "ifsampling is dynamic then\n",
      "Re-compute k-NN graph of remain X\n",
      "end\n",
      "until (NN−score(X )=0 )∨(|trainsample|≤k);\n",
      "ifXis not Null then\n",
      "trainsample =trainsample∪(t,m,s) −Nets (X)\n",
      "end\n",
      "end\n",
      "parameters are derived from the designed tSNE model. Inlier\n",
      "data points are interpolated into the designed tSNE modelusing local IDW-Interpolation method. The identiﬁed outlierswhich are no more related to outliers of tSNE they can beplaced randomly into the predeﬁned locations. In otherwords,the identiﬁed outliers have some relation with outliers of tSNEthat are placed near to them by using parameter r\n",
      "close . The\n",
      "parameter ryNN is the minimum distance from data points to\n",
      "outliers and from outlier to outlier, and the outlier locationsare determined by the outlier\n",
      "placement algorithm using ryNN\n",
      "based on the heuristic proposed by Andrey Boytsov et.al [22].\n",
      "C.k-NN accuracy\n",
      "The newly added data samples should be embedded among\n",
      "the training samples of similar characteristics. To evaluate\n",
      "this, for each newly added data sample k-NN accuracy is\n",
      "calculated in embedding space, and observe the percentageofkneighbors which are having the similar characteristics.\n",
      "Thek-NN accuracy typically depends on the parameter k\n",
      "value which is considered as ﬁxed in our experimentation. Forinstance, the smaller value of kwill give good performance\n",
      "accuracy and while increasing the kvalue performance accu-\n",
      "racy will decrease. The selection of parameter kis also playing\n",
      "a paramount role in k-NN accuracy measure. The relationship\n",
      "between k-value and accuracy is shown in the ﬁgure 5.\n",
      "V. E\n",
      "V ALUA TION\n",
      "To evaluate the proposed method, The results of proposed\n",
      "methods are compared with the state-of-the-art technique\n",
      "67\n",
      "LION-tSNE. We performed experiments on ﬁve numerical\n",
      "datasets that represent applications of various domains.\n",
      "The section Adescribes the datasets which are used in\n",
      "our experiments. The experimental conﬁguration explored insection B. In section C, we present the experimental evaluation\n",
      "among the results of our proposed methods based LION-tSNEand random sampling based LION-tSNE. The evaluation isdone by considering three different initial solutions such asPCA, MDS and random for designing initial tSNE model.\n",
      "A. Datasets\n",
      "In our experimental evaluation, we have considered ﬁve\n",
      "differently characterized datasets such as IRIS dataset, Breast-\n",
      "Cancer dataset, Wine dataset, MNIST dataset, and Olivetti-Faces dataset. In Olivetti-Faces dataset, ten images of oneindividual were considered as small variation in viewpoint,large variation in expression, and addition of glasses. Thetable Iprovides the detailed description of all ﬁve datasets.\n",
      "Dataset Name Size Dimensions Classes\n",
      "IRIS 150 4 3\n",
      "Breast Cancer 569 30 2\n",
      "Wine 178 13 3\n",
      "MNSIT 70K 784 10\n",
      "Olivetti faces 400 10304 40\n",
      "Table I: Overview of datasets along with their size, dimensions, and classes\n",
      "B. Experimental conﬁguration\n",
      "In all of our experimental evaluation, we have used three\n",
      "different initial solutions for embedding of BH-tSNE that arePCA, MDS, and random. The PCA and MDS initial solutionsreduce the divergence cost. The datasets with more than 30dimensions, their dimensionality is reduced to 30 dimensionsby PCA. This process speeds up the computation of theprobability distribution of the input similarity and suppressessome noise. The results of BH-tSNE algorithm are shown inscatter-plot representation. For all the datasets, there is a classinformation of each data point which can be used only forvisual clarity of a scatter-plot. The data point class informationis not used for any other purposes.\n",
      "Due to the computational complexity of proposed method\n",
      "and BH-tSNE algorithm (i.e., computational complexityO(NlogN) ) we have considered limited dataset size for\n",
      "computation. For our experimentation, we have considered themaximum dataset size as 10K. There is a scope for expansionof dataset size by extending the implementation in distributedenvironment.\n",
      "Table IIprovides the parameter settings of our experimental\n",
      "evaluation. The parameter perplexity represents the effective\n",
      "number of neighbors for each data point. For instance, thesmall value of perplexity creates subgroups within the same\n",
      "cluster of tSNE results. In contrast to small, the large valueof it does not maintain the clear separation between twoclusters of tSNE results. In both cases, there is a lack ofvisual clarity, the empirical studies state that the perplexityvalue between 5 to 70 gives a good visual representation oftSNE results. The parameter dist\n",
      "per represents the overall\n",
      "percentage of train sample points considered as inliers. For\n",
      "example, if we take dist peras 95th percentile that means out\n",
      "of 100 points, 95 points are considered as inliers and remaining5 points are considered as an outlier. The parameter kplays\n",
      "an important role in the computation of k-NN accuracy of the\n",
      "data. The effect of parameter kis shown in the ﬁgure 5.I t\n",
      "is clear that when there is an increment in kvalue, accuracy\n",
      "decrease monotonically. The ﬁgure 5 shows the k-NN accuracy\n",
      "of including and excluding outliers. Because of the paperlimitations, we are evaluating the proposed k-NN sampling\n",
      "method results using only one dataset (i.e., MNIST dataset ofsize 10K).\n",
      "Parameter Value\n",
      "Perplexity 5-3 0\n",
      "rxNN atdist perc 100\n",
      "Fixed k-value for k-NN accuracy 10, 3(for Olivetti-face dataset)\n",
      "Table II: Parameters setting for the experimental setup\n",
      "Figure 5: Relationship between k-value of k-NN accuracy and k-NN accuracy\n",
      "which is deﬁned on optimal train sample size.\n",
      "C. Results\n",
      "Inﬁgure 6, we show the experimental train sample selection\n",
      "results of proposed k-NN sampling methods(with static and\n",
      "dynamic k-NN graph update) and random sampling on IRIS\n",
      "dataset. For clear 2D visual clarity, we have chosen IRISdataset because original dimensionality of the IRIS dataset isfour which is very small. So, the direct 2D visual representa-tion of 4D data (i.e., IRIS data) does not show the huge varia-tion in scatter-plot representation. The train\n",
      "sample selection\n",
      "(i.e., represented by blue star scatter-plot point) results in sub-ﬁgure 6(a)and6(b)of the proposed k-NN sampling methods\n",
      "clearly shows the uniﬁed sampling of complete data. In sub-ﬁgure 6(c), the results of random sampling shows that some\n",
      "regions of the data are not covered by selected sample andrandom sampling is not consistent. Due to the inconsistency ofrandom sampling, there is a lack of representativeness whichcauses the inefﬁcient result of tSNE.\n",
      "The relationship between train\n",
      "sample selection and ﬁxed\n",
      "k-NN accuracy is shown in the ﬁgure 7. The ﬁgure 7(a)and\n",
      "7(b)shows the realization of k-NN accuracy with including\n",
      "and excluding outliers and train sample size. In both the\n",
      "ﬁgures, it is clear that the k-NN accuracy of both including and\n",
      "68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) Static k-NN sampling\n",
      " (b) Dynamic k-NN sampling\n",
      " (c) Random sampling\n",
      "Figure 6: Training sample selection (i.e., represented by blue star scatter-plot point) from IRIS dataset using proposed k-NN sampling methods(with static\n",
      "and dynamic k-NN graph update) and random sampling\n",
      "excluding outliers is monotonically reducing while increasing\n",
      "thek-value of k-NN sampling. The corresponding random\n",
      "sampling results also realized beside proposed sampling results(i.e, titled as Random sampling). There is a small ﬂuctuation inthe accuracy curve which is caused by additional train\n",
      "sample\n",
      "selection from the remaining list using (t,m,s)-Nets. Thetrain\n",
      "sample size is also decreasing while increasing the k-\n",
      "value of k-NN sampling. The relationship between k-NN\n",
      "accuracy of including and excluding outliers is deﬁned bythe Pearson correlation coefﬁcient [30]. For all datasets, thecorrelation coefﬁcient is approximately equal to one whichmeans both including and excluding outliers k-NN accuracies\n",
      "are more similar to each other (that means optimal solutionhas very small number of outliers). The relationship betweentrain\n",
      "sample size and ﬁxed k-NN accuracy is derived by\n",
      "linear regression model [31] which is shown in the ﬁgure7(c),7(d)and 7(e). The linear regression model for three\n",
      "different situations will produce good regression score (i.e.,above 0.9). Empirically, the regression score is more than 0.5\n",
      "means the y variable (i.e., y-axis) is well deﬁned by variablex (i.e., x-axis).\n",
      "The ﬁgure 8 gives the 2D visual representation of MNIST\n",
      "dataset of 10K points with PCA based initial solution. Sub-ﬁgures 8(a),8(d)and8(g)shows train\n",
      "sample scatter-plots\n",
      "which is obtained by BH-tSNE model. Sub-ﬁgure 8(a)and\n",
      "8(d)shows the scatter-plots of proposed methods; static and\n",
      "dynamic k-NN sampling. The ﬁgure 8(g)shows the scatter-\n",
      "plot of random sampling of a size equivalent to the staticvariant of the proposed method. The ﬁgures 8(b),8(c),8(e),\n",
      "8(f),8(h)and 8(i)shows the scatter-plots of LION-tSNE\n",
      "of all three methods. The ﬁgures 8(b),8(e)and8(h)gives\n",
      "the scatter-plots of three methods before power parameteroptimization. The ﬁgures 8(c),8(f)and8(i)shows the scatter-\n",
      "plots of all three methods after power parameter optimization.After power optimization, the scatter-plots of the proposedmethods are more clear and effective than the state-of-the-arttechnique.\n",
      "In our result analysis, we are comparing the k-NN accura-\n",
      "cies of proposed k-NN sampling and random sampling based\n",
      "LION-tSNE results. We are also using three different initialsolutions such as PCA, MDS and random. The empirical studystates that the results of MDS are computationally slower thanthe results of PCA. The PCA and MDS initial solutions reducethe computational complexity of the original tSNE modelby early convergence. The accuracies of proposed k-NN and\n",
      "random sampling based LION-tSNE results are assessed withbaseline accuracy. Baseline accuracy is obtained by applyingtSNE on overall data. The baseline accuracies of ﬁve datasetsare shown in the table III. The k-NN accuracy comparison of\n",
      "proposed k-NN and random sampling based LION-tSNE re-\n",
      "sults on MNIST dataset with three different initial solutions isshown in the ﬁgure 9 along with baseline accuracy. The ﬁgure\n",
      "9(a),9(b)and9(c)clearly shows that the k-NN accuracies\n",
      "of proposed k-NN sampling based LION-tSNE outperforms\n",
      "existing state-of-the-art that is random sampling based LION-tSNE in all three initial solution constraint. Also, proposedmethods accuracy results are almost equal to baseline results.For some datasets such as IRIS, Breast-Cancer, and Wine,the proposed methods accuracy result is more than baselineaccuracy. The k-NN accuracies of proposed k-NN and random\n",
      "sampling based LION-tSNE results on ﬁve datasets with threedifferent initial solutions are listed in table IVand V.\n",
      "Dataset Dataset size Baseline Accuracy\n",
      "IRIS 150 0.95238\n",
      "Breast-Cancer 569 0.91282\n",
      "Wine 178 0.69943\n",
      "MNIST Digits 10000 0.93298999\n",
      "Olivetti face 400 0.92416\n",
      "Table III: k-NN accuracy of static variant of proposed method for various\n",
      "datasets\n",
      "Tabel IVshows the static k-NN and random sampling based\n",
      "k-NN accuracies of LION-tSNE on all ﬁve datasets. For all\n",
      "ﬁve datasets with all three initial solution constraints and sametrain\n",
      "sample size, the static k-NN sampling based LION-tSNE\n",
      "is giving better k-NN accuracies than random sampling based\n",
      "LION-tSNE. In tabel V, thek-NN accuracies of dynamic\n",
      "k-NN and random sampling based LION-tSNE are listed\n",
      "on all ﬁve datasets. Here also, the dynamic k-NN sampling\n",
      "based LION-tSNE is reporting better k-NN accuracies than\n",
      "random sampling based LION-tSNE in all circumstances. Ifwe compare the results of both proposed methods whichare shown in table IVand V. The static method gives more\n",
      "accuracy than dynamic one. We also observed that in mostof the cases both static and dynamic k-NN sampling methods\n",
      "selecting optimal train\n",
      "sample at k-value either one or two of\n",
      "k-NN graph.\n",
      "69\n",
      "(a) Realization of train sample size and k-NN accuracy of\n",
      "static k-NN and random sampling based LION-tSNE results\n",
      "with ten different k-values\n",
      "(b) Realization of train sample size and k-NN accuracy of\n",
      "dynamic k-NN and random sampling based LION-tSNE results\n",
      "with ten different k-values\n",
      "(c) Relationship between train sample size and\n",
      "k-NN accuracy of static k-NN sampling method\n",
      "(d) Relationship between train sample size and\n",
      "k-NN accuracy of random sampling method\n",
      "(e) Relationship between train sample size and\n",
      "k-NN accuracy of dynamic k-NN sampling\n",
      "method\n",
      "Figure 7: Realization of train sample size and ﬁxed k-NN accuracy, relationship between train sample size and ﬁxed k-NN accuracy with liner regression\n",
      "model.\n",
      "Dataset Initial Train Static k-NN sampling Ran.sampling\n",
      "solution\n",
      "typesamplesize Opt.k-value Opt.power k-NN\n",
      "accurOpt.power k-NN\n",
      "accur\n",
      "IRIS PCA 54 3 2 0.96068 1 0.94246\n",
      "MDS 44 5 2 0.96382 9 0.94701\n",
      "Random 66 2 2 0.95586 5 0.94082\n",
      "Breast PCA 108 9 46 0.91265 49 0.90530\n",
      "Cancer MDS 80 14 15 0.91569 11 0.89557\n",
      "Random 63 19 3 0.91058 32 0.90353\n",
      "Wine PCA 70 3 1 0.69943 3 0.68563\n",
      "MDS 36 7 6 0.70505 44 0.67901\n",
      "Random 50 5 2 0.71348 1 0.70568\n",
      "MNIST PCA 5169 1 49 0.91387 45 0.90855\n",
      "Digits MDS 4586 2 48 0.91882 49 0.89989\n",
      "Random 5118 1 48 0.91879 45 0.90869\n",
      "Olivetti PCA 217 1 31 0.91583 41 0.88719\n",
      "Face MDS 220 1 39 0.915 40 0.89313\n",
      "Random 221 1 40 0.92333 26 0.89983\n",
      "Table IV: k-NN accuracy results of LION-tSNE for static k-NN and random\n",
      "sampling on ﬁve datasets\n",
      "The proposed k-NN sampling methods are statistically\n",
      "signiﬁcant than the random one which is proved by the\n",
      "statistical method pairwise t-test [32]. For pairwise t-test, wehave repeated out experiments 25 times on all ﬁve datasets.Each time we have collected optimal LION-tSNE accuracyresults of proposed methods as well as corresponding LION-tSNE results of random sampling method. The proposed k-\n",
      "NN sampling based LION-tSNE gives more consistent resultsthan random one which is shown in the ﬁgure 10. The ﬁgure\n",
      "10(a) and 10(b) clearly shows the superiority of proposed k-\n",
      "NN sampling over random sampling. The ﬁgure 10(c) shows\n",
      "the results of static k-NN sampling are more accurate than\n",
      "dynamic k-NN sampling. For evaluation, we have conducted\n",
      "three pairwise t-test such as static k-NN vs random, dynamicDataset Initial Train Dynamic k-NN sampling Ran.sampling\n",
      "solution\n",
      "typesamplesize Opt.k-value Opt.power k-NN\n",
      "accurOpt.power k-NN\n",
      "accur\n",
      "IRIS PCA 49 2 18 0.96013 1 0.95546\n",
      "MDS 49 2 6 0.96293 3 0.9531\n",
      "Random 49 2 21 0.96223 32 0.95174\n",
      "Breast PCA 241 2 26 0.90984 9 0.90598\n",
      "Cancer MDS 191 2 30 0.91232 19 0.90896\n",
      "Random 152 5 5 0.91669 43 0.90371\n",
      "Wine PCA 68 3 1 0.71404 4 0.69548\n",
      "MDS 89 1 5 0.71129 1 0.69017\n",
      "Random 69 3 1 0.70617 35 0.69550\n",
      "MNIST PCA 5000 1 49 0.91592 49 0.90700\n",
      "Digits MDS 5000 1 48 0.92032 49 0.91221\n",
      "Random 5000 1 42 0.91324 49 0.90984\n",
      "Olivetti PCA 200 1 41 0.91 49 0.87786\n",
      "Face MDS 200 1 46 0.91916 47 0.87955\n",
      "Random 200 1 23 0.92499 22 0.88416\n",
      "Table V: k-NN accuracy results of LION-tSNE for static k-NN and random\n",
      "sampling on ﬁve datasets\n",
      "k-NN vs random and static k-NN vs dynamic k-NN. From\n",
      "all three t-test for all ﬁve datasets, we have obtained the\n",
      "positive values for t-statistic and p-values are less than 0.05.\n",
      "The sample pairwise t-test of MNIST dataset is shown inthe table VI. The empirical studies states that the p-value of\n",
      "pairwise t-test is less than 0.05then the results are statistically\n",
      "signiﬁcant. Therefore, the proposed k-NN sampling methods\n",
      "are more consistent and efﬁcient.\n",
      "t-test pair t-statistic value p-value\n",
      "(static k-NN, random) 10.908428 4.38×10−11\n",
      "(dynamic k-NN, random) 5.233352 1.15×10−5\n",
      "(static k-NN, dynamic k-NN) 12.709989 1.88×10−12\n",
      "Table VI: Pairwise t-test results on MNIST dataset of size 10K for evaluation\n",
      "of statistical signiﬁcance\n",
      "70\n",
      "(a) Proposed static k-NN sampling based 2D\n",
      "visual representation of tSNE\n",
      "(b) Proposed static k-NN sampling based 2D vi-\n",
      "sual representation of LION-tSNE before power\n",
      "parameter optimization\n",
      "(c) Proposed static k-NN sampling based 2D\n",
      "visual representation of LION-tSNE after powerparameter optimization\n",
      "(d) Proposed dynamic k-NN sampling based 2D\n",
      "visual representation of tSNE\n",
      "(e) Proposed dynamic k-NN sampling based\n",
      "2D visual representation of LION-tSNE beforepower parameter optimization\n",
      "(f) Proposed dynamic k-NN sampling based 2D\n",
      "visual representation of LION-tSNE after powerparameter optimization\n",
      "(g) Random sampling based 2D visual represen-\n",
      "tation of tSNE\n",
      "(h) Random sampling based 2D visual repre-\n",
      "sentation of LION-tSNE before power parameteroptimization\n",
      "(i) Random sampling based 2D visual representation\n",
      "of LION-tSNE after power parameter optimization\n",
      "Figure 8: Three different 2D visual representation of tSNE and LION-tSNE on MNIST dataset of size 10K\n",
      "VI. C ONCLUSION AND FUTURE WORK\n",
      "This paper deals with adding new data samples into the\n",
      "existing tSNE environment to overcome the scalability is-\n",
      "sues. We proposed two new approaches such as static anddynamic k-NN graph update methods (i.e., k-NN sampling)\n",
      "for good tSNE model design. This method selects the uniﬁedtrain\n",
      "samples from entire data. The proposed methods give\n",
      "good samples to overcome the lack of representativeness.Our experiments on ﬁve datasets show that proposed methodsoutperform existing state-of-the-art techniques for adding newdata point into an existing tSNE environment and also handledoutliers.\n",
      "Proposed method deals with numerical datasets which can\n",
      "be enhanced in future to categorical and mixed datasets.Due to the computational complexity, the proposed methodis suitable to limited dataset size. To overcome this limitation,it can be implemented in distributed environment. In ourwork, we are assuming some of the data points are outliersfrom tSNE enviroment; instead of that we would like to dealoutliers within the tSNE itself by introducing threshold value.Also, to get clear insights from the obtained NN\n",
      "score and\n",
      "MNN score, it is good to view the visual representation of\n",
      "k-NN graph. Therefore, we would like to use the method of\n",
      "Yang Zhang et.al. [33] to visualize the relationship among thedata points.\n",
      "R\n",
      "EFERENCES\n",
      "[1] L. van der Maaten, E. Postma, and H. van den Herik. Dimensionality\n",
      "reduction: A comparative review. Technical report, Tilburg University\n",
      "Technical Report, TiCC-TR 2009-005, 2009.\n",
      "[2] J. V enna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski. Information\n",
      "retrieval perspective to nonlinear dimensionality reduction for datavisualization. Journal of Machine Learning Research, 11:451-490, 2010.\n",
      "[3] K. Bunte, M. Biehl, and B. Hammer. A general framework for di-\n",
      "mensionality reducing data visualization mapping. Neural Computation,24(3):771-804,2012.\n",
      "[4] M. Partridge and R. Calvo. Fast dimensionality reduction and Simple\n",
      "PCA. Intelligent Data Analysis, 2(3):292-298, 1997.\n",
      "[5] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.\n",
      "[6] Trevor F. Cox,Michael A. A. Cox. Multidimensional scaling, 2nd edi-\n",
      "tion,Chapman & Hall/CRC, 2001.\n",
      "71\n",
      "(a) MDS initial solution\n",
      " (b) PCA initial solution\n",
      " (c) Random initial solution\n",
      "Figure 9: Performance evaluation of proposed static k-NN sampling based LION-tSNE accuracy results with baseline accuracy for three different initial\n",
      "solutions of MNIST dataset of size 10K.\n",
      "(a) Static k-NN vs random sampling\n",
      " (b) Dynamic k-NN vs random sampling\n",
      " (c) Static vs dynamic k-NN sampling\n",
      "Figure 10: Comparison of three different paired results of LION-tSNE over 25 intervals on MNIST dataset of size 10K. The initial solution is PCA based.\n",
      "[7] J. Tenenbaum, V . da Silva, and J. Langford. A global geometric framework\n",
      "for nonlinear dimensionality reduction. Science, 290:2319-2323, 2000.\n",
      "[8] J.W. Sammon. A nonlinear mapping for data structure analysis . IEEE\n",
      "Transactions on Computers, 18(5):401-409, 1969.\n",
      "[9] P . Demartines, J.Herault, Curvilinear Component Analysis: A Self-\n",
      "Organizing Neural Networks for nonlinear mapping of datasets, IEEE\n",
      "Trans. Neural Networks, vol.8, pp.1197-1206, 1997.\n",
      "[10] S. T. Roweis and L. K. Saul., Nonlinear dimensionality reduction by\n",
      "locally linear embedding, Science, 290: 2323 - 2326, 2000.\n",
      "[11] M. Belkin and P . Niyogi. Laplacian eigenmaps for dimensionality\n",
      "reduction and data representation . Neural Computation, 15:1373-1396,\n",
      "2003.\n",
      "[12] K. Weinberger and L. K. Saul., An introduction to nonlinear dimension-\n",
      "ality reduction by maximum variance unfolding. In Proceedings of theNational Conference on Artiﬁcial Intelligence, pages 1683-1686, Boston,MA, 2006.\n",
      "[13] Geoffrey E Hinton and Sam T. Roweis. Stochastic Neighbor Embedding ,\n",
      "Advances in Neural Information Processing Systems 15, pages 857-864.MIT Press, 2003.\n",
      "[14] Laurens van der Maaten and Geoffrey Hinton, Visualizing Data using\n",
      "t-SNE, Journal of Machine Learning Research, 9(Nov):2579-2605, 2008.\n",
      "[15] Wentian Li, Jane E. Cerise, Yaning Yang, and Henry Han. Application\n",
      "of t-SNE to human genetic data, J. Bioinform. Comput. Biol. (2017), p.1750017, 10.1142/S0219720017500172\n",
      "[16] Minh Nguyen, Sanjay Purushotham, Hien To, and Cyrus Shahabi. m-\n",
      "TSNE: A Framework for Visualizing High-Dimensional Multivariate TimeSeries, In V AHC2016 Workshop on Visual Analytics in Healthcare inconjunction with AMIA 2016, 2016.\n",
      "[17] Walid M. Abdelmoula, Benjamin Balluff, Sonja Englert, Jouke Dijkstra,\n",
      "Marcel J. T. Reinders, Axel Walch, Liam A. McDonnell, and BoudewijnP . F. Lelieveldt. Data-driven identiﬁcation of prognostic tumor subpopu-\n",
      "lations using spatially mapped t-SNE of mass spectrometry imaging data,Proceedings of the National Academy of Sciences, 113(43):12244-12249,October 2016.\n",
      "[18] T. Kohonen. Self-organization and associative memory: 3 rd edition.\n",
      "Springer-V erlag New York, Inc., New York, NY , USA, 1989.\n",
      "[19] Laurens van der Maaten, Learning a Parametric Embedding by Preserv-\n",
      "ing Local Structure , In International Conference on Artiﬁcial Intelligence\n",
      "and Statistics, pages 384391, 2009.\n",
      "[20] Andrej Gisbrecht, Alexander Schulz, and Barbara Hammer. Parametric\n",
      "nonlinear dimensionality reduction using kernel t-SNE, Neurocomputing,147:7182, January 2015.[21] N. Pezzotti, B. P . F. Lelieveldt, L. v d Maaten, T. Hllt, E. Eisemann,\n",
      "and A. Vilanova. Approximated and User Steerable tSNE for Progressive\n",
      "Visual Analytics, IEEE Transactions on Visualization and ComputerGraphics, 23(7):17391752, July 2017.\n",
      "[22] Boytsov A, Fouquet F, Hartmann T, LeTraon Y . Visualizing\n",
      "and exploring dynamic high-dimensional datasets with LION-tSNE,arXiv:1708.04983, 2017.\n",
      "[23] J.A. Cook, I. Sutskever, A. Mnih, and G.E. Hinton. Visualizing similarity\n",
      "data with a mixture of maps, In Proceedings of the 11 th InternationalConference on Artiﬁcial Intelligence and Statistics, volume 2, pages 6774,2007.\n",
      "[24] T. Kollig, A. Keller, Efﬁcient multidimensional sampling, Computer\n",
      "Graphics Forum, vol. 21, no. 3, 2002.\n",
      "[25] Martin D. Buhmann. Radial Basis Functions: Theory and Imple-\n",
      "mentations. Cambridge University Press, July 2003. Google-Books-ID:TRMf53opzlsC.\n",
      "[26] Donald Shepard. A Two-dimensional Interpolation Function for\n",
      "Irregularly-spaced Data, In Proceedings of the 1968 23rd ACM NationalConference, ACM 68, pages 517524, New York, NY , USA, 1968.\n",
      "[27] L. van der Maaten, Accelerating t-sne using tree-based algorithms,J .\n",
      "Mach. Learn. Res., vol. 15, pp. 32213245, 2014.\n",
      "[28] Ching Tarn, Yinan Zhang and Ye Feng, Sampling Clustering,\n",
      "arXiv:1806.08245v1 [cs.CV], June 2018.\n",
      "[29] Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A Fast Learn-\n",
      "ing Algorithm for Deep Belief Nets, Neural Computation, 18(7):15271554,May 2006.\n",
      "[30] B. Jacob, J. Chen, Y . Huang, I. Cohen, Pearson correlation coefﬁcient in\n",
      "Noise Reduction in Speech Processing, Berlin, Germany:Springer-V erlag,pp. 1-4, 2009.\n",
      "[31] D.C. Montgomery,Design and Analysis of Experiments (3rd Edition),\n",
      "Wiley, New York (1991)\n",
      "[32] L. J. Williams and H. Abdi, Fishers least signiﬁcance difference (LSD)\n",
      "test, in Encyclopedia of Research Design. Thousand Oaks, 2010, pp.491494.\n",
      "[33] Y . Zhang, Y . Wang, S. Parthasarathy, ”Visualizing attributed graphs\n",
      "via terrain metaphor”, Proc. 23rd ACM SIGKDD Int. Conf. Knowl.Discovery Data Mining, pp. 1325-1334, 2017.\n",
      "[34] Albert Kim , Eric Blais , Aditya Parameswaran , Piotr Indyk , Sam\n",
      "Madden , Ronitt Rubinfeld, Rapid sampling for visualizations withordering guarantees, Proceedings of the VLDB Endowment, v.8 n.5,p.521-532, January 2015.\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each page and extract the text\n",
    "for page in pdf.pages:\n",
    "  print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49547e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
